#### Initial Setup

- Download the [stanford corenlp zip](https://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip).
- Extract it to a folder named `stanford-corenlp` in this directory

---

### stanford-openie.sh

Running Open IE on large corpus 

```bash
./stanford-openie.sh <path_to_corpus_file> <output_file> <coref(optional)>
```

If coref is given as third argument   

- coreference resolution along with openIE
- Leads to better triplets (free of pronouns)
- increases time drastically

**Output format**

The generated output file will contain triplets of the format on each line

`confidence_score: (subject; relation; object)`

---

### utils.py
Contains various utility functions used by other scripts.

**Functions**

- **replace_wh_word_with_blank**
    + Args:
        * question
- **create_hypothesis**
    + Args:
        * question with wh words replaced with blanks
        * choice (the possible answer)
- **stanford_ie**
    + Args:
        * str: string on which openie is to be run
        * coref=False: whehter to perform coreference resolution or not.
    + A java process is spawned on every call to this function.
    + Good If openie is to be run on just one sentence.
    + Slow for large number of strings.
    + 2 sec per call. 1.5 sec is just to load annotators

- **stanford_ie_v2**
    + Args:
        * str: string on which openie is to be run
        * coref=False: whether to perform coreference resolution or not.
    + Assumes a openIE server is running at `http://localhost:9000` with appropriate annotators loaded.
    + Queries the server.
    + Good If openie is to be run on many sentences.
    + Fastest. As annotators are already loaded (server).
    + Uses: class StanfordNLP
        * requirements: `pip install StanfordCoreNLP`

- **process_entity_relations**
    + Args 
        * list of openIE relation strings of form `1.000: (subject; relation; object)`
    + Converts list of `1.000: (subject; relation; object)` -> list of `[subject, relation, object]`
    
---

### graph.py

Contains the Graph class used to store the knowledge graphs.

Creating graph:

- `python graph.py <openie_file> <stem>`
    + **openie_file**: file containing triplets generated by the openIE
    + **stem**: if `True` stemming of the words in triplets is done before creating graph. No stemming is done if `False`
    + Graph is generated quite quickly. So haven't added the functionality to dump it yet.


---

### create_graph_for_qa.py

Runs openIE on the QA file, extract triplets, generate graph for each hypothesis of all questions and then dump the into a pickle object.

- `export http_proxy=""`
- Assumes a openIE server is running at `http://localhost:9000` with appropriate annotators loaded.
    ```bash
    java -mx10g -cp "stanford-corenlp/*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators "tokenize,ssplit,pos,lemma,depparse,natlog,coref,openie" -port 9000 -timeout 30000
    ```
- `python create_graph_for_qa.py <dump_file_name> <coref>`
    + **dump_file_name**: The name of the pickle dump file
    + **coref**: whether to perform coreference resolution or not (True/False)

---

### graph_to_text.py

- Read the pickle dump generated by previous file
- Generate a human readable/understandable text (`stdin`) and tsv file (`out.tsv`)

- `python graph_to_text.py <dump_file_name> <tsv_file_name>`
    + **dump_file_name**: the dump file to read
    + **tsv_file_name**: name of the tsv file to be generated

---

### run.py

Read the openie_triplets corresponding to corpus from a file, read the pickle dump on qa hypothesis graphs, score them using ``compared_graph`` function defined inside Graph Class

`python run.py <corpus_triplets> <stem> <qa_graph_dump> <output_file>`

- Args:
    + **corpus_triplets**: path to file containing openie triplets of the corpus
    + **stem**: whether to do stemming or not while creating corpus graph
    + **qa_graph_dump**: pickle dump of list graph objects for each hypothesis of every question
    + **output_file**: path to output file in which the predictions/scores will be written